# This code was originally generated by ChatGPT with the following prompt:
# "write a web spider in python that identifies all the unique internal links on a page"
# This code doesn't work very well, because even if a page is returned with a 200 OK code,
# the program will display the "page not fetched" error if no internal links are detected
# that match the simplistic "internal links" suggestion criteria
import requests
from bs4 import BeautifulSoup


def spider(url):
    page = requests.get(url)
    if page.status_code == 200:
        soup = BeautifulSoup(page.content, "html.parser")
        links = set()
        for link in soup.find_all("a"):
            href = link.get("href")
            if href and url in href:
                links.add(href)
        return links
    else:
        return None


url = "https://www.example.com"
internal_links = spider(url)

if internal_links:
    print("Internal links:")
    for link in internal_links:
        print(link)
else:
    print("Could not fetch the page")
