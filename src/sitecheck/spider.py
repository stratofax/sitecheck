# This code was originally generated by ChatGPT with the following prompt:
# "write a web spider in python that identifies all the unique internal links on a page"
# I further updated the code using Sourcery then reconfigured the spider function.

import requests
from bs4 import BeautifulSoup


def spider(url: str, links: set) -> int:
    page = requests.get(url)
    status = page.status_code
    if status == 200:
        soup = BeautifulSoup(page.content, "html.parser")
        for link in soup.find_all("a"):
            href = link.get("href")
            # This doesn't work for site root links, like '/about.html'
            # if href and url in href:
            links.add(href)
    return status


url = "https://erlenepsyd.com/"

links = set()
status = spider(url, links)
if status == 200:
    print(f"Links found (response {status}):")
    for link in links:
        print(link)
else:
    print(f"Could not fetch the page - response {status}")
